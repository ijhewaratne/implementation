#!/usr/bin/env python3
"""
Comprehensive Integration Tests for Advanced ADK Features
Part of Phase 6: Advanced ADK Features
"""

import sys
import os
import time
import json
import tempfile
import shutil
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime, timedelta
import unittest
from unittest.mock import Mock, patch, MagicMock

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

# Import all advanced ADK components
try:
    from enhanced_agents_advanced import AdvancedADKAgent, AgentMemoryManager
    from advanced_tool_chaining import AdvancedToolChainer
    from advanced_tool_monitoring import ADKToolMonitor
    from advanced_agent_monitoring import ADKAgentPerformanceMonitor
    from advanced_usage_analytics import ADKUsageAnalytics
    from advanced_adk_dashboards import ADKDashboard
    ADK_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Some ADK components not available: {e}")
    # Create mock classes for testing
    class MockComponent:
        def __init__(self, *args, **kwargs):
            pass
        def __getattr__(self, name):
            return lambda *args, **kwargs: {}
        def close(self):
            pass
    
    AdvancedADKAgent = MockComponent
    AgentMemoryManager = MockComponent
    AdvancedToolChainer = MockComponent
    ADKToolMonitor = MockComponent
    ADKAgentPerformanceMonitor = MockComponent
    ADKUsageAnalytics = MockComponent
    ADKDashboard = MockComponent
    ADK_COMPONENTS_AVAILABLE = False

class TestAdvancedADKIntegration(unittest.TestCase):
    """Comprehensive integration tests for advanced ADK features."""
    
    def setUp(self):
        """Set up test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.test_data_dir = os.path.join(self.temp_dir, 'test_data')
        os.makedirs(self.test_data_dir, exist_ok=True)
        
        # Mock ADK Agent if not available
        self.mock_adk_agent = Mock()
        self.mock_adk_agent.name = "TestAgent"
        self.mock_adk_agent.system_prompt = "Test system prompt"
        self.mock_adk_agent.tools = []
        
        # Initialize test components
        self.agent_memory_manager = AgentMemoryManager()
        self.tool_chainer = AdvancedToolChainer()
        self.tool_monitor = ADKToolMonitor()
        self.performance_monitor = ADKAgentPerformanceMonitor()
        self.usage_analytics = ADKUsageAnalytics()
        self.dashboard = ADKDashboard()
        
        print(f"Test environment set up in: {self.temp_dir}")
    
    def tearDown(self):
        """Clean up test environment."""
        # Close database connections
        try:
            self.agent_memory_manager.close()
            self.usage_analytics.close()
            self.dashboard.close()
        except:
            pass
        
        # Remove temp directory
        shutil.rmtree(self.temp_dir, ignore_errors=True)
        print("Test environment cleaned up")
    
    def test_advanced_agent_capabilities_integration(self):
        """Test integration of advanced agent capabilities."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing Advanced Agent Capabilities Integration")
        
        # Test agent memory and context
        print("   üìã Testing agent memory and context...")
        
        # Add test interactions
        test_interactions = [
            {
                "request": "analyze district heating for Parkstra√üe",
                "result": {"success": True, "response_time": 2.5, "agent_response": "Analysis completed"},
                "user_id": "test_user_1",
                "tags": ["district_heating", "analysis"]
            },
            {
                "request": "compare heating scenarios for Parkstra√üe",
                "result": {"success": True, "response_time": 3.2, "agent_response": "Comparison completed"},
                "user_id": "test_user_1",
                "tags": ["comparison", "scenarios"]
            },
            {
                "request": "generate report for Parkstra√üe",
                "result": {"success": True, "response_time": 5.8, "agent_response": "Report generated"},
                "user_id": "test_user_2",
                "tags": ["report", "generation"]
            }
        ]
        
        for interaction in test_interactions:
            self.agent_memory_manager.add_interaction(
                interaction["request"],
                interaction["result"],
                interaction["user_id"],
                interaction["tags"]
            )
        
        # Test memory retrieval
        relevant_memory = self.agent_memory_manager.get_relevant_memory("analyze district heating", "test_user_1")
        
        self.assertIsInstance(relevant_memory, dict)
        self.assertIn("session_context", relevant_memory)
        self.assertIn("user_preferences", relevant_memory)
        self.assertIn("learning_insights", relevant_memory)
        
        print("   ‚úÖ Agent memory and context working correctly")
        
        # Test advanced tool chaining
        print("   üìã Testing advanced tool chaining...")
        
        # Create test workflow
        test_workflow = [
            {
                "tool_name": "data_exploration",
                "parameters": {"street_name": "Parkstra√üe"},
                "output": "exploration_result",
                "dependencies": []
            },
            {
                "tool_name": "district_heating_analysis",
                "parameters": {"street_name": "Parkstra√üe", "use_exploration_data": True},
                "output": "dh_analysis_result",
                "dependencies": ["exploration_result"]
            },
            {
                "tool_name": "heat_pump_feasibility",
                "parameters": {"street_name": "Parkstra√üe", "use_exploration_data": True},
                "output": "hp_analysis_result",
                "dependencies": ["exploration_result"]
            },
            {
                "tool_name": "scenario_comparison",
                "parameters": {"dh_result": True, "hp_result": True},
                "output": "comparison_result",
                "dependencies": ["dh_analysis_result", "hp_analysis_result"]
            }
        ]
        
        # Execute tool chain
        chain_result = self.tool_chainer.execute_tool_chain(test_workflow)
        
        self.assertIsInstance(chain_result, dict)
        self.assertIn("success", chain_result)
        self.assertIn("execution_metadata", chain_result)
        
        print("   ‚úÖ Advanced tool chaining working correctly")
        
        # Test adaptive behavior (simulated)
        print("   üìã Testing adaptive behavior...")
        
        # Simulate learning from interactions
        learning_insights = self.agent_memory_manager.learning_memory.get_learning_insights()
        
        self.assertIsInstance(learning_insights, dict)
        self.assertIn("total_interactions", learning_insights)
        self.assertIn("average_response_time", learning_insights)
        self.assertIn("success_rate", learning_insights)
        
        print("   ‚úÖ Adaptive behavior working correctly")
        
        print("‚úÖ Advanced agent capabilities integration test passed")
    
    def test_enhanced_tool_integration(self):
        """Test integration of enhanced tool capabilities."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing Enhanced Tool Integration")
        
        # Test tool orchestration
        print("   üìã Testing tool orchestration...")
        
        # Create parallel tool execution
        parallel_tools = [
            {
                "tool_name": "data_exploration",
                "parameters": {"street_name": "Parkstra√üe"},
                "output": "exploration_1"
            },
            {
                "tool_name": "data_exploration",
                "parameters": {"street_name": "Hauptstra√üe"},
                "output": "exploration_2"
            },
            {
                "tool_name": "performance_monitoring",
                "parameters": {"monitor_type": "system_health"},
                "output": "system_status"
            }
        ]
        
        # Execute parallel tools
        parallel_result = self.tool_chainer.execute_parallel_tools(parallel_tools)
        
        self.assertIsInstance(parallel_result, dict)
        self.assertIn("success", parallel_result)
        self.assertIn("execution_metadata", parallel_result)
        
        print("   ‚úÖ Tool orchestration working correctly")
        
        # Test tool monitoring
        print("   üìã Testing tool monitoring...")
        
        # Start monitoring
        self.tool_monitor.start_monitoring()
        
        # Simulate tool usage
        for i in range(10):
            tool_data = {
                "tool_name": f"test_tool_{i % 3}",
                "execution_time": 1.0 + i * 0.1,
                "success": i % 4 != 0,  # 75% success rate
                "memory_usage": 50 + i * 2,
                "user_id": f"user_{i % 2}"
            }
            self.tool_monitor.track_tool_usage(tool_data)
        
        # Stop monitoring and get analytics
        self.tool_monitor.stop_monitoring()
        analytics = self.tool_monitor.get_comprehensive_analytics()
        
        self.assertIsInstance(analytics, dict)
        self.assertIn("performance_analytics", analytics)
        self.assertIn("usage_tracking", analytics)
        self.assertIn("predictive_maintenance", analytics)
        
        print("   ‚úÖ Tool monitoring working correctly")
        
        # Test tool intelligence (simulated)
        print("   üìã Testing tool intelligence...")
        
        # Get tool performance insights
        performance_insights = self.tool_monitor.get_performance_insights()
        
        self.assertIsInstance(performance_insights, dict)
        self.assertIn("insights", performance_insights)
        
        print("   ‚úÖ Tool intelligence working correctly")
        
        print("‚úÖ Enhanced tool integration test passed")
    
    def test_analytics_and_monitoring_integration(self):
        """Test integration of analytics and monitoring."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing Analytics and Monitoring Integration")
        
        # Test performance monitoring
        print("   üìã Testing performance monitoring...")
        
        # Start performance monitoring
        self.performance_monitor.start_monitoring()
        
        # Simulate agent operations
        agents = ['EnergyPlannerAgent', 'CentralHeatingAgent', 'DecentralizedHeatingAgent']
        for i in range(50):
            for agent in agents:
                operation_data = {
                    "agent_name": agent,
                    "operation_type": f"operation_{i % 5}",
                    "execution_time": 1.0 + (i % 10) * 0.2,
                    "success": i % 8 != 0,  # 87.5% success rate
                    "memory_usage": 100 + (i % 20) * 5
                }
                self.performance_monitor.track_agent_operation(operation_data)
        
        # Stop monitoring and get analytics
        self.performance_monitor.stop_monitoring()
        performance_analytics = self.performance_monitor.get_comprehensive_analytics()
        
        self.assertIsInstance(performance_analytics, dict)
        self.assertIn("efficiency_analytics", performance_analytics)
        self.assertIn("delegation_analytics", performance_analytics)
        self.assertIn("response_time_analysis", performance_analytics)
        
        print("   ‚úÖ Performance monitoring working correctly")
        
        # Test usage analytics
        print("   üìã Testing usage analytics...")
        
        # Simulate usage events
        features = ['district_heating_analysis', 'heat_pump_feasibility', 'scenario_comparison']
        users = ['user_1', 'user_2', 'user_3']
        
        for i in range(100):
            usage_data = {
                "user_id": users[i % len(users)],
                "feature": features[i % len(features)],
                "duration": 2.0 + (i % 10) * 0.5,
                "success": i % 10 != 0,  # 90% success rate
                "performance_metrics": {
                    "memory_usage_mb": 100 + (i % 50),
                    "cpu_usage_percent": 20 + (i % 30)
                }
            }
            self.usage_analytics.track_feature_usage(features[i % len(features)], usage_data)
        
        # Get usage analytics
        usage_analysis = self.usage_analytics.analyze_user_behavior()
        
        self.assertIsInstance(usage_analysis, dict)
        self.assertIn("usage_patterns", usage_analysis)
        self.assertIn("user_segments", usage_analysis)
        self.assertIn("behavioral_insights", usage_analysis)
        
        print("   ‚úÖ Usage analytics working correctly")
        
        # Test dashboard functionality
        print("   üìã Testing dashboard functionality...")
        
        # Update some metrics for dashboard
        self.dashboard.real_time_monitor.update_metric('cpu_usage', 45.2, '%', 'healthy', 'stable')
        self.dashboard.real_time_monitor.update_metric('memory_usage', 67.8, '%', 'healthy', 'up')
        self.dashboard.real_time_monitor.update_metric('response_time', 2.3, 's', 'healthy', 'stable')
        
        # Get dashboard data
        dashboard_data = self.dashboard.get_dashboard_data("comprehensive")
        
        self.assertIsInstance(dashboard_data, dict)
        self.assertIn("live_metrics", dashboard_data)
        self.assertIn("performance_trends", dashboard_data)
        self.assertIn("usage_patterns", dashboard_data)
        self.assertIn("predictive_insights", dashboard_data)
        
        # Test HTML rendering
        html_content = self.dashboard.render_dashboard("comprehensive")
        
        self.assertIsInstance(html_content, str)
        self.assertIn("<!DOCTYPE html>", html_content)
        self.assertIn("ADK System Dashboard", html_content)
        
        print("   ‚úÖ Dashboard functionality working correctly")
        
        print("‚úÖ Analytics and monitoring integration test passed")
    
    def test_end_to_end_advanced_workflow(self):
        """Test complete advanced workflow."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing End-to-End Advanced Workflow")
        
        # Test complete advanced analysis workflow
        print("   üìã Testing complete advanced analysis workflow...")
        
        # Step 1: Initialize all components
        print("      Step 1: Initializing components...")
        
        # Start monitoring
        self.performance_monitor.start_monitoring()
        self.tool_monitor.start_monitoring()
        
        # Step 2: Simulate user request
        print("      Step 2: Simulating user request...")
        
        user_request = "analyze heating options for Parkstra√üe and compare district heating vs heat pumps"
        user_id = "test_user_integration"
        
        # Step 3: Process through agent memory
        print("      Step 3: Processing through agent memory...")
        
        # Add to memory
        self.agent_memory_manager.add_interaction(
            user_request,
            {"success": True, "response_time": 0.1, "agent_response": "Request received"},
            user_id,
            ["integration_test", "heating_analysis"]
        )
        
        # Step 4: Execute advanced tool chain
        print("      Step 4: Executing advanced tool chain...")
        
        advanced_workflow = [
            {
                "tool_name": "data_exploration",
                "parameters": {"street_name": "Parkstra√üe", "analysis_type": "comprehensive"},
                "output": "exploration_data",
                "dependencies": []
            },
            {
                "tool_name": "district_heating_analysis",
                "parameters": {"street_name": "Parkstra√üe", "detailed_analysis": True},
                "output": "dh_analysis",
                "dependencies": ["exploration_data"]
            },
            {
                "tool_name": "heat_pump_feasibility",
                "parameters": {"street_name": "Parkstra√üe", "detailed_analysis": True},
                "output": "hp_analysis",
                "dependencies": ["exploration_data"]
            },
            {
                "tool_name": "scenario_comparison",
                "parameters": {"comparison_type": "detailed", "include_costs": True},
                "output": "comparison_result",
                "dependencies": ["dh_analysis", "hp_analysis"]
            },
            {
                "tool_name": "report_generation",
                "parameters": {"report_type": "comprehensive", "include_recommendations": True},
                "output": "final_report",
                "dependencies": ["comparison_result"]
            }
        ]
        
        # Execute workflow
        workflow_result = self.tool_chainer.execute_tool_chain(advanced_workflow)
        
        self.assertIsInstance(workflow_result, dict)
        self.assertIn("success", workflow_result)
        self.assertIn("execution_metadata", workflow_result)
        
        # Step 5: Track tool usage
        print("      Step 5: Tracking tool usage...")
        
        for step in advanced_workflow:
            tool_data = {
                "tool_name": step["tool_name"],
                "execution_time": 2.0 + len(step["parameters"]) * 0.5,
                "success": True,
                "memory_usage": 100 + len(step["tool_name"]) * 2,
                "user_id": user_id
            }
            self.tool_monitor.track_tool_usage(tool_data)
        
        # Step 6: Track agent performance
        print("      Step 6: Tracking agent performance...")
        
        agent_operations = [
            {"agent_name": "EnergyPlannerAgent", "operation_type": "delegation", "execution_time": 1.2, "success": True},
            {"agent_name": "CentralHeatingAgent", "operation_type": "analysis", "execution_time": 8.5, "success": True},
            {"agent_name": "DecentralizedHeatingAgent", "operation_type": "analysis", "execution_time": 6.3, "success": True},
            {"agent_name": "ComparisonAgent", "operation_type": "comparison", "execution_time": 4.1, "success": True}
        ]
        
        for operation in agent_operations:
            self.performance_monitor.track_agent_operation(operation)
        
        # Step 7: Track usage analytics
        print("      Step 7: Tracking usage analytics...")
        
        usage_events = [
            {"user_id": user_id, "feature": "district_heating_analysis", "duration": 8.5, "success": True},
            {"user_id": user_id, "feature": "heat_pump_feasibility", "duration": 6.3, "success": True},
            {"user_id": user_id, "feature": "scenario_comparison", "duration": 4.1, "success": True},
            {"user_id": user_id, "feature": "report_generation", "duration": 3.2, "success": True}
        ]
        
        for event in usage_events:
            self.usage_analytics.track_feature_usage(event["feature"], event)
        
        # Step 8: Generate comprehensive analytics
        print("      Step 8: Generating comprehensive analytics...")
        
        # Stop monitoring
        self.performance_monitor.stop_monitoring()
        self.tool_monitor.stop_monitoring()
        
        # Get comprehensive analytics
        performance_analytics = self.performance_monitor.get_comprehensive_analytics()
        tool_analytics = self.tool_monitor.get_comprehensive_analytics()
        usage_analytics = self.usage_analytics.get_comprehensive_analytics()
        dashboard_data = self.dashboard.get_dashboard_data("comprehensive")
        
        # Verify all analytics are working
        self.assertIsInstance(performance_analytics, dict)
        self.assertIsInstance(tool_analytics, dict)
        self.assertIsInstance(usage_analytics, dict)
        self.assertIsInstance(dashboard_data, dict)
        
        # Step 9: Test dashboard rendering
        print("      Step 9: Testing dashboard rendering...")
        
        html_dashboard = self.dashboard.render_dashboard("comprehensive")
        self.assertIsInstance(html_dashboard, str)
        self.assertIn("<!DOCTYPE html>", html_dashboard)
        
        # Step 10: Verify end-to-end integration
        print("      Step 10: Verifying end-to-end integration...")
        
        # Check that all components have data
        memory_insights = self.agent_memory_manager.learning_memory.get_learning_insights()
        self.assertGreater(memory_insights["total_interactions"], 0)
        
        tool_performance = self.tool_monitor.get_performance_analytics()
        self.assertIsInstance(tool_performance, dict)
        
        usage_patterns = self.usage_analytics.analyze_user_behavior()
        self.assertIn("usage_patterns", usage_patterns)
        
        print("      ‚úÖ All components integrated successfully")
        
        print("‚úÖ End-to-end advanced workflow test passed")
    
    def test_performance_and_reliability(self):
        """Test performance and reliability of the integrated system."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing Performance and Reliability")
        
        # Test system performance under load
        print("   üìã Testing system performance under load...")
        
        start_time = time.time()
        
        # Simulate high load
        for i in range(100):
            # Track agent operations
            operation_data = {
                "agent_name": f"TestAgent_{i % 5}",
                "operation_type": f"operation_{i % 10}",
                "execution_time": 0.1 + (i % 5) * 0.1,
                "success": i % 20 != 0,  # 95% success rate
                "memory_usage": 50 + (i % 10) * 5
            }
            self.performance_monitor.track_agent_operation(operation_data)
            
            # Track tool usage
            tool_data = {
                "tool_name": f"test_tool_{i % 3}",
                "execution_time": 0.2 + (i % 3) * 0.1,
                "success": i % 15 != 0,  # 93.3% success rate
                "memory_usage": 30 + (i % 8) * 2,
                "user_id": f"load_test_user_{i % 10}"
            }
            self.tool_monitor.track_tool_usage(tool_data)
            
            # Track usage analytics
            usage_data = {
                "user_id": f"load_test_user_{i % 10}",
                "feature": f"feature_{i % 5}",
                "duration": 1.0 + (i % 10) * 0.2,
                "success": i % 12 != 0,  # 91.7% success rate
                "performance_metrics": {
                    "memory_usage_mb": 80 + (i % 20),
                    "cpu_usage_percent": 15 + (i % 25)
                }
            }
            self.usage_analytics.track_feature_usage(f"feature_{i % 5}", usage_data)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"   ‚úÖ Load test completed in {execution_time:.2f} seconds")
        self.assertLess(execution_time, 10.0, "System should handle 100 operations in under 10 seconds")
        
        # Test system reliability
        print("   üìã Testing system reliability...")
        
        # Get analytics after load test
        performance_analytics = self.performance_monitor.get_comprehensive_analytics()
        tool_analytics = self.tool_monitor.get_comprehensive_analytics()
        usage_analytics = self.usage_analytics.get_comprehensive_analytics()
        
        # Verify data integrity
        self.assertIsInstance(performance_analytics, dict)
        self.assertIsInstance(tool_analytics, dict)
        self.assertIsInstance(usage_analytics, dict)
        
        # Test error handling
        print("   üìã Testing error handling...")
        
        # Test with invalid data
        try:
            invalid_workflow = [{"invalid": "data"}]
            result = self.tool_chainer.execute_tool_chain(invalid_workflow)
            # Should handle gracefully
            self.assertIsInstance(result, dict)
        except Exception as e:
            # Should not crash the system
            self.assertIsInstance(e, Exception)
        
        print("   ‚úÖ Error handling working correctly")
        
        print("‚úÖ Performance and reliability test passed")
    
    def test_data_consistency_and_persistence(self):
        """Test data consistency and persistence across components."""
        if not ADK_COMPONENTS_AVAILABLE:
            self.skipTest("ADK components not available")
        
        print("\nüß™ Testing Data Consistency and Persistence")
        
        # Test data consistency
        print("   üìã Testing data consistency...")
        
        # Add consistent data across components
        test_user_id = "consistency_test_user"
        test_feature = "consistency_test_feature"
        
        # Add to agent memory
        self.agent_memory_manager.add_interaction(
            "test request",
            {"success": True, "response_time": 2.0, "agent_response": "test response"},
            test_user_id,
            ["consistency_test"]
        )
        
        # Add to usage analytics
        usage_data = {
            "user_id": test_user_id,
            "feature": test_feature,
            "duration": 2.0,
            "success": True,
            "performance_metrics": {"memory_usage_mb": 100, "cpu_usage_percent": 25}
        }
        self.usage_analytics.track_feature_usage(test_feature, usage_data)
        
        # Add to tool monitoring
        tool_data = {
            "tool_name": test_feature,
            "execution_time": 2.0,
            "success": True,
            "memory_usage": 100,
            "user_id": test_user_id
        }
        self.tool_monitor.track_tool_usage(tool_data)
        
        # Verify data consistency
        memory_insights = self.agent_memory_manager.learning_memory.get_learning_insights()
        usage_patterns = self.usage_analytics.analyze_user_behavior()
        tool_analytics = self.tool_monitor.get_performance_analytics()
        
        # All should have data
        self.assertGreater(memory_insights["total_interactions"], 0)
        self.assertIn("usage_patterns", usage_patterns)
        self.assertIsInstance(tool_analytics, dict)
        
        print("   ‚úÖ Data consistency verified")
        
        # Test persistence (simulated)
        print("   üìã Testing data persistence...")
        
        # Get initial counts
        initial_memory_interactions = memory_insights["total_interactions"]
        initial_usage_events = usage_patterns["usage_patterns"]["total_usage_events"]
        
        # Simulate system restart by reinitializing components
        # (In real scenario, this would test database persistence)
        print("   ‚úÖ Data persistence test completed (simulated)")
        
        print("‚úÖ Data consistency and persistence test passed")

def run_integration_tests():
    """Run all integration tests."""
    print("üöÄ Advanced ADK Integration Tests")
    print("=" * 60)
    
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestAdvancedADKIntegration)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Print summary
    print("\n" + "=" * 60)
    print("INTEGRATION TEST SUMMARY")
    print("=" * 60)
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")
    
    if result.failures:
        print("\nFAILURES:")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback}")
    
    if result.errors:
        print("\nERRORS:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback}")
    
    if result.wasSuccessful():
        print("\nüéâ All integration tests passed!")
        return True
    else:
        print("\n‚ùå Some integration tests failed!")
        return False

if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)
